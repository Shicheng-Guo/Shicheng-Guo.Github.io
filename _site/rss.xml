<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
        <title>Shirin's playgRound</title>
        <description>Shirin's playgRound - Shirin Glander</description>
        <link>https://shiring.github.io</link>
        <atom:link href="https://shiring.github.io/rss.xml" rel="self" type="application/rss+xml" />
        <lastBuildDate>Sat, 23 Dec 2017 16:52:09 +0100</lastBuildDate>
        <pubDate>Sat, 23 Dec 2017 16:52:09 +0100</pubDate>
        <ttl>60</ttl>


			<item>
				<title>Explore Predictive Maintenance with flexdashboard</title>
				        
					<description>&lt;p&gt;I have written the following post about &lt;a href=&quot;https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/&quot;&gt;Predictive Maintenance and flexdashboard&lt;/a&gt; at my company &lt;a href=&quot;https://blog.codecentric.de/en/&quot;&gt;codecentric&lt;/a&gt;’s blog:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Predictive Maintenance is an increasingly popular strategy associated with Industry 4.0; it uses advanced analytics and machine learning to optimize machine costs and output (see Google Trends plot below).
A common use-case for Predictive Maintenance is to proactively monitor machines, so as to predict when a check-up is needed to reduce failure and maximize performance. In contrast to traditional maintenance, where each machine has to undergo regular routine check-ups, Predictive Maintenance can save costs and reduce downtime. A machine learning approach to such a problem would be to analyze machine failure over time to train a supervised classification model that predicts failure. Data from sensors and weather information is often used as features in modeling.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;With flexdashboard RStudio provides a great way to create interactive dashboards with R. It is an easy and very fast way to present analyses or create story maps. Here, I have used it to demonstrate different analysis techniques for Predictive Maintenance. It uses Shiny run-time to create interactive content.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Continue reading at https://blog.codecentric.de/en/2017/11/explore-predictive-maintenance-flexdashboard/&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://blog.codecentric.de/files/2017/10/dashboard_screenshot.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 02 Nov 2017 00:00:00 +0100</pubDate>
				<link>http://localhost:4000/flexdashboard/2017/11/02/predictive_maintenance_flexdashboard</link>
				<guid isPermaLink="true">http://localhost:4000/flexdashboard/2017/11/02/predictive_maintenance_flexdashboard</guid>
			</item>
		
			<item>
				<title>Blockchain &amp; distributed ML - my report from the data2day conference</title>
				        
					<description>&lt;p&gt;&lt;img src=&quot;https://www.data2day.de/common/images/konferenzen/data2day2017.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Yesterday and today I attended the &lt;a href=&quot;www.data2day.de&quot;&gt;data2day&lt;/a&gt;, a conference about Big Data, Machine Learning and Data Science in Heidelberg, Germany. Topics and workshops covered a range of topics surrounding (big) data analysis and Machine Learning, like Deep Learning, Reinforcement Learning, TensorFlow applications, etc. Distributed systems and scalability were a major part of a lot of the talks as well, reflecting the growing desire to build bigger and more complex models that can’t (or would take too long to) run on a single computer. Most of the application examples were built in Python but one talk by Andreas Prawitt was specifically titled “Using R for Predictive Maintenance: an example from the TRUMPF Laser GmbH”. I also saw quite a few graphs that were obviously made with ggplot!&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;
&lt;p lang=&quot;de&quot; dir=&quot;ltr&quot;&gt;
Guten Morgen auf der &lt;a href=&quot;https://twitter.com/data2day&quot;&gt;@data2day&lt;/a&gt; Kommt uns doch mal am Stand besuchen :-) &lt;a href=&quot;https://t.co/YK46ACdNj9&quot;&gt;pic.twitter.com/YK46ACdNj9&lt;/a&gt;
&lt;/p&gt;
— codecentric AG (@codecentric) &lt;a href=&quot;https://twitter.com/codecentric/status/912928993279606784&quot;&gt;September 27, 2017&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The keynote lecture on Wednesday about &lt;strong&gt;Blockchains for AI&lt;/strong&gt; was given by Trent McConaghy. &lt;a href=&quot;https://www.sitepen.com/blog/2017/09/21/blockchain-basics/&quot;&gt;Blockchain technology&lt;/a&gt; is based on a decentralized system of storing and validating data and changes in data. It experiences a huge hype at the moment but it is only starting to gain track in Data Science and Machine Learning as well. I therefore found it a very fitting topic for the keynote lecture! Trent and his colleagues at &lt;a href=&quot;www.bigchaindb.com&quot;&gt;BigchainDB&lt;/a&gt; are implementing an “internet-scale blockchain database for the world” - the Interplanetary Database (IPDB).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“IPDB is a blockchain database that offers decentralized control, immutability and the creation and trading of digital assets. […] As a database for the world, IPDB offers decentralized control, strong governance and universal accessibility. IPDB relies on “caretaker” organizations around the world, who share responsibility for managing the network and governing the IPDB Foundation. Anyone in the world will be able to use IPDB. […]” &lt;a href=&quot;https://blog.bigchaindb.com/ipdb-announced-as-public-planetary-scale-blockchain-database-7a363824fc14&quot;&gt;https://blog.bigchaindb.com/ipdb-announced-as-public-planetary-scale-blockchain-database-7a363824fc14&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;He presented a number of examples where blockchain technology for decentralized data storage/access can be beneficial to Machine Learning and AI, like exchanging data from self-driving cars, of online market places and for generating art with computers. You can learn more about him &lt;a href=&quot;https://blog.oceanprotocol.com/from-ai-to-blockchain-to-data-meet-ocean-f210ff460465&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;
It's always been about the data.&lt;br /&gt;Announcing Ocean.&lt;a href=&quot;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;\#AI&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/Blockchain?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;\#Blockchain&lt;/a&gt; &lt;a href=&quot;https://twitter.com/oceanprotocol?ref_src=twsrc%5Etfw&quot;&gt;@OceanProtocol&lt;/a&gt;&lt;a href=&quot;https://t.co/Do4XNn3ucN&quot;&gt;https://t.co/Do4XNn3ucN&lt;/a&gt;
&lt;/p&gt;
— Trent McConaghy (@trentmc0) &lt;a href=&quot;https://twitter.com/trentmc0/status/909793166416662528?ref_src=twsrc%5Etfw&quot;&gt;September 18, 2017&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The other talks were a mix of high- and low-level topics: from introductions to machine learning, Apache Spark and data analysis with Python to (distributed) data streaming with Kappa architecture or Apache Kafka, containerization with Docker and Kubernetes, data archiving with Apache Cassandra, relevance tuning with Solr and much more. While I spent most of the time at my company’s conference stand, I did hear three of the talks. I summarize each of them below…&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;
&lt;p lang=&quot;de&quot; dir=&quot;ltr&quot;&gt;
Tag 2 auf der &lt;a href=&quot;https://twitter.com/data2day?ref_src=twsrc%5Etfw&quot;&gt;@data2day&lt;/a&gt; kommt am Stand vorbei, wir haben noch ein paar T-Shirts und Softwerker für euch :-) &lt;a href=&quot;https://t.co/xyG8Leg3lF&quot;&gt;pic.twitter.com/xyG8Leg3lF&lt;/a&gt;
&lt;/p&gt;
— codecentric AG (@codecentric) &lt;a href=&quot;https://twitter.com/codecentric/status/913301091755941888?ref_src=twsrc%5Etfw&quot;&gt;September 28, 2017&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Scalable Machine Learning with Apache Spark for Fraud Detection&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this first talk I heard, Dr. Patrick Baier and Dr. Stanimir Dragiev presented their work at &lt;a href=&quot;www.zalando.de/&quot;&gt;Zalando&lt;/a&gt;. They built a scalable machine learning framework with Apache Spark, Scala and AWS to assess and predict fraud in online transactions. &lt;a href=&quot;www.zalando.de/&quot;&gt;Zalando&lt;/a&gt; is a German online store that sells clothes, shoes and accessories. Normally, they allow registered customers to buy via invoice, i.e. they receive their ordered items before they pay them. This leaves them vulnerable to fraud where item are not paid for. The goal of their data science team is to use customer and basket data to obtain a probability score for how likely a transaction is going to be fraudulent. High-risk payment options, like invoice, can then be disabled in transactions with high fraud probability. To build and run such machine learning models, the Zalando data science team uses a combination of Spark, Scala, R, AWS, SQL, Python, Docker, etc. In their workflow, they use a combination of static and dynamic features, imputing missing values and building a decision model. In order to scale their modeling workflow to process more requests, use more data in training, update models more frequently and/or run more models, they described a workflow that uses Spark, Scala and Amazon Web Services (AWS). Spark’s machine learning library can be used for modeling and scaled horizontally by increasing the number of clusters on which to run the models. Scala provides multi-threading functionality and AWS is used for storing data in S3 and extending computation power depending on changing needs. Finally, they include a model inspector into their workflow to assure comparability of training and test data and check that the model is behaving as expected. Problems that they are dealing with include highly unbalanced data (which is getting even worse the better their models work as they keep reducing the number of fraud cases), delayed labeling due to the long process of the transactions, seasonality in data.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Sparse Data: Don’t Mind the Gap!&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this talk, my colleagues from &lt;a href=&quot;www.codecentric.de&quot;&gt;codecentric&lt;/a&gt; Dr. Daniel Pape and Dr. Michael Plümacher showed an example from ad targeting of how to deal with sparse data. Sparse data occurs in many areas, e.g. as rare events over a long period of time or in areas where there are many items and few occurrences per item, like in recommender systems or in natural language processing (NLP). In ad targeting, the measure of success is the rate of the click-through rate (CRT): this is the number of clicks on a given advertisement displayed to a user on a website divided by the total number of advertisements, or impressions. Because financial revenue comes from a high CTR, advertisements should be placed in a way that maximizes their chance of being clicked, i.e. we want to recommend advertisements for specific users that match their interests or are of actual relevance. Sparsity come into play with ad targeting because the number of clicks is very low compared to two metrics: a) from all the potential ads that a user could see, only a small proportion is actually shown to her/him and b) of the ads that a user sees, she/he only clicks on very few. This means that, a CTR matrix of advertisements x targets will have very few combinations that have been clicked (the mean CTR is 0.003) and contain many missing values. The approach they took was to impute the missing values and predict for each target/user the most similar ads from the imputed CTR matrix. This approach worked well for a reasonably large data set but it didn’t perform so well with smaller (and therefore even sparser) data. They then talked about alternative approaches, like grouping users and/or ads into groups in order to reduce the sparsity of the data. Their take-home messages were that 1) there is no one-size-fits-all solution, what works depends on the context and 2) if the underlying data is of bad quality, the results will be sub-optimal - no matter how sophisticated the model.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Distributed TensorFlow with Kubernetes&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the third talk, another colleague of mine from &lt;a href=&quot;www.codecentric.de&quot;&gt;codecentric&lt;/a&gt;, Jakob Karalus, explained in detail how to set up a distributed machine learning modelling set-up with &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;. TensorFlow is used to build neural networks in a graph-based manner. Distributed and parallel machine learning can be necessary when training big neural networks with a lot of training data, very deep neural networks, with complex parameters, grid search for hyper-parameter tuning, etc. A good way to build neural networks in a controlled and stable environment is to use &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; containers. Kubernetes is a container orchestration tool that can set up distribution of nodes from our TensorFlow modeling container. Setting up this distributed system is quite complex, though and Jakob recommended to try to stay on one CPU/GPU as long as possible.&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;
&lt;p lang=&quot;de&quot; dir=&quot;ltr&quot;&gt;
Verteiltes Deep Learning mit TensorFlow und Kubernetes - &lt;a href=&quot;https://twitter.com/krallistic&quot;&gt;@krallistic&lt;/a&gt; auf der &lt;a href=&quot;https://twitter.com/data2day&quot;&gt;@data2day&lt;/a&gt; &lt;a href=&quot;https://t.co/5AGJdhL5U1&quot;&gt;pic.twitter.com/5AGJdhL5U1&lt;/a&gt;
&lt;/p&gt;
— codecentric AG (@codecentric) &lt;a href=&quot;https://twitter.com/codecentric/status/913041395128111105&quot;&gt;September 27, 2017&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

</description>
				<pubDate>Thu, 28 Sep 2017 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/conferences/data2day/2017/09/28/data2day</link>
				<guid isPermaLink="true">http://localhost:4000/conferences/data2day/2017/09/28/data2day</guid>
			</item>
		
			<item>
				<title>From Biology to Industry. A Blogger’s Journey to Data Science.</title>
				        
					<description>&lt;p&gt;Today, I have given a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”&lt;/p&gt;

&lt;p&gt;I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.&lt;/p&gt;

&lt;p&gt;My slides can be found here: &lt;a href=&quot;https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science&quot;&gt;https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;from-biology-to-industry-a-bloggers-journey-to-data-science-1-638.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Wed, 20 Sep 2017 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/blogging/2017/09/20/webinar_biology_to_data_science</link>
				<guid isPermaLink="true">http://localhost:4000/blogging/2017/09/20/webinar_biology_to_data_science</guid>
			</item>
		
			<item>
				<title>Why I use R for Data Science - An Ode to R</title>
				        
					<description>&lt;p&gt;I have written a &lt;a href=&quot;https://shirinsplayground.netlify.com/2017/09/ode_to_r/&quot;&gt;blog post about why I love R&lt;/a&gt; and prefer it to other languages. The post is on &lt;a href=&quot;https://www.shirin-glander.de/&quot;&gt;my new site&lt;/a&gt;, but since it isn’t on R-bloggers yet I am also posting the &lt;a href=&quot;https://shirinsplayground.netlify.com/2017/09/ode_to_r/&quot;&gt;link here&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Working in Data Science, I often feel like I have to justify using R over Python. And while I do use Python for running scripts in production, I am much more comfortable with the R environment. Basically, whenever I can, I use R for prototyping, testing, visualizing and teaching. But because personal gut-feeling preference isn’t a very good reason to give to (scientifically minded) people, I’ve thought a lot about the pros and cons of using R. This is what I came up with why I still prefer R.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;I can’t stress enough how much I appreciate all the people who are involved in the R-community; who write packages, tutorials, blogs, who share information, provide support and who think about how to make data analysis easy, more convenient and - dare I say - fun!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://shirinsplayground.netlify.com/2017/09/ode_to_r/&quot;&gt;Continue reading…&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Tue, 19 Sep 2017 00:00:00 +0200</pubDate>
				<link>http://localhost:4000/blogging/2017/09/19/ode_to_r</link>
				<guid isPermaLink="true">http://localhost:4000/blogging/2017/09/19/ode_to_r</guid>
			</item>
		

</channel>
</rss>